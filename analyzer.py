#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Excelå­—æ®µåˆ†æå¼•æ“ - Claude Code Skillç‰ˆæœ¬
æ”¯æŒå¤šæºå­—æ®µæ˜ å°„é…ç½®ã€äº¤äº’å¼å­¦ä¹ å’ŒAIæ‰¹é‡æ˜ å°„ç”Ÿæˆ
"""

import os
import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
from collections import Counter

# å¯¼å…¥AIæ˜ å°„å™¨
try:
    from ai_mapper import AIFieldMapper
    AI_MAPPER_AVAILABLE = True
except ImportError:
    AI_MAPPER_AVAILABLE = False
    print("âš ï¸ AIæ˜ å°„å™¨æœªåŠ è½½ï¼Œæ‰¹é‡å­¦ä¹ åŠŸèƒ½ä¸å¯ç”¨")

# å¯¼å…¥è´¨é‡æ£€æŸ¥å™¨
try:
    from mapping_validator import MappingValidator
    VALIDATOR_AVAILABLE = True
except ImportError:
    VALIDATOR_AVAILABLE = False
    print("âš ï¸ è´¨é‡æ£€æŸ¥å™¨æœªåŠ è½½ï¼Œæ˜ å°„è´¨é‡éªŒè¯åŠŸèƒ½ä¸å¯ç”¨")


class FieldMappingManager:
    """å­—æ®µæ˜ å°„ç®¡ç†å™¨ - æ”¯æŒå¤šæºé…ç½®"""

    def __init__(self, skill_dir: Path):
        self.skill_dir = skill_dir
        self.mappings_dir = skill_dir / 'field_mappings'
        self.combined_mappings = {}
        self.load_all_mappings()

    def load_all_mappings(self):
        """åŠ è½½æ‰€æœ‰æ˜ å°„é…ç½®æ–‡ä»¶"""
        if not self.mappings_dir.exists():
            return

        for json_file in self.mappings_dir.glob('*.json'):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    mappings = config.get('mappings', {})
                    # åˆå¹¶åˆ°æ€»æ˜ å°„è¡¨ï¼ˆååŠ è½½çš„ä¼šè¦†ç›–å…ˆåŠ è½½çš„ï¼‰
                    self.combined_mappings.update(mappings)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æ˜ å°„æ–‡ä»¶å¤±è´¥ {json_file.name}: {e}")

    def get_mapping(self, cn_field: str) -> Optional[Dict]:
        """è·å–å­—æ®µæ˜ å°„"""
        return self.combined_mappings.get(cn_field)

    def add_custom_mapping(self, cn_field: str, en_name: str, group: str, dtype: str, description: str = ""):
        """æ·»åŠ è‡ªå®šä¹‰æ˜ å°„å¹¶ä¿å­˜"""
        custom_file = self.mappings_dir / 'custom.json'

        # è¯»å–ç°æœ‰é…ç½®
        if custom_file.exists():
            with open(custom_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
        else:
            config = {
                "domain": "custom",
                "description": "ç”¨æˆ·è‡ªå®šä¹‰å­—æ®µæ˜ å°„",
                "version": "1.0",
                "mappings": {},
                "learn_history": []
            }

        # æ·»åŠ æ–°æ˜ å°„
        config['mappings'][cn_field] = {
            "en_name": en_name,
            "group": group,
            "dtype": dtype,
            "description": description or f"{cn_field}çš„è‡ªå®šä¹‰æ˜ å°„"
        }

        # è®°å½•å­¦ä¹ å†å²
        config['learn_history'].append({
            "cn_field": cn_field,
            "en_name": en_name,
            "learned_at": datetime.now().isoformat()
        })

        # ä¿å­˜
        with open(custom_file, 'w', encoding='utf-8') as f:
            json.dump(config, ensure_ascii=False, indent=2, fp=f)

        # æ›´æ–°å†…å­˜ä¸­çš„æ˜ å°„
        self.combined_mappings[cn_field] = config['mappings'][cn_field]

    def get_phrase_to_token_dict(self) -> Dict[str, str]:
        """ç”ŸæˆçŸ­è¯­åˆ°tokençš„å­—å…¸ï¼ˆå…¼å®¹æ—§ç‰ˆï¼‰"""
        result = {}
        for cn_field, mapping in self.combined_mappings.items():
            result[cn_field] = mapping['en_name']
        return result

    def batch_learn_fields(self, unknown_fields: List[str], df: pd.DataFrame = None) -> Dict[str, Dict[str, str]]:
        """
        ä½¿ç”¨AIæ‰¹é‡å­¦ä¹ æœªçŸ¥å­—æ®µ

        Args:
            unknown_fields: æœªçŸ¥å­—æ®µåˆ—è¡¨
            df: åŒ…å«è¿™äº›å­—æ®µçš„DataFrame

        Returns:
            ç”Ÿæˆçš„å­—æ®µæ˜ å°„
        """
        if not AI_MAPPER_AVAILABLE:
            print("âŒ AIæ˜ å°„å™¨ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæ‰¹é‡å­¦ä¹ ")
            return {}

        ai_mapper = AIFieldMapper()
        mappings = ai_mapper.batch_analyze_fields(unknown_fields, df)

        # æ‰¹é‡ä¿å­˜åˆ°custom.json
        for cn_field, mapping in mappings.items():
            self.add_custom_mapping(
                cn_field=cn_field,
                en_name=mapping['en_name'],
                group=mapping['group'],
                dtype=mapping['dtype'],
                description=mapping['description']
            )

        return mappings


class ExcelAnalyzer:
    """Excelå­—æ®µåˆ†æå™¨ - å¢å¼ºç‰ˆ"""

    def __init__(self, skill_dir: Optional[Path] = None):
        if skill_dir is None:
            skill_dir = Path(__file__).parent

        self.mapping_manager = FieldMappingManager(skill_dir)
        self.phrase_to_token = self.mapping_manager.get_phrase_to_token_dict()

    def is_csv_file(self, file_path: Path) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºCSVæ–‡ä»¶"""
        return file_path.suffix.lower() in ['.csv', '.txt']

    def load_sheet(self, file_path: Path, sheet_name: str = None) -> pd.DataFrame:
        """è¯»å–æŒ‡å®šå·¥ä½œè¡¨æˆ–CSVæ–‡ä»¶"""
        if self.is_csv_file(file_path):
            # CSVæ–‡ä»¶
            df = pd.read_csv(file_path, dtype=str, encoding='utf-8-sig')
        else:
            # Excelæ–‡ä»¶
            df = pd.read_excel(file_path, sheet_name=sheet_name, dtype=str)

        df = df.map(lambda x: str(x).strip() if pd.notna(x) else x)

        # è¯†åˆ«æ—¶é—´åˆ—
        time_cols = [c for c in df.columns if ('æ—¶é—´' in str(c)) or ('æ—¥æœŸ' in str(c))]
        for c in time_cols:
            df[c] = pd.to_datetime(df[c], errors='coerce')

        # è¯†åˆ«æ•°å€¼åˆ—
        num_like = [c for c in df.columns if any(k in str(c) for k in [
            'é‡‘é¢', 'ä¿è´¹', 'NCD', 'è´¹ç”¨', 'æ€»è´¹ç”¨', 'èµ”æ¬¾', 'èµ”ä»˜',
            'æ¡ˆä»¶æ•°', 'é¢‘åº¦', 'ç³»æ•°', 'è¯„åˆ†', '(ä¸‡)', 'ï¼ˆä¸‡ï¼‰', 'ç‡'
        ])]
        for c in num_like:
            s = df[c].str.replace(',', '', regex=False)
            df[c] = pd.to_numeric(s, errors='coerce')

        return df

    def summarize_columns(self, df: pd.DataFrame, topn: int = 10) -> Dict[str, Dict[str, Any]]:
        """ç”Ÿæˆåˆ—çº§æ‘˜è¦"""
        summary: Dict[str, Dict[str, Any]] = {}
        for c in df.columns:
            col = df[c]
            n = len(col)
            na = int(col.isna().sum())
            non_na = n - na
            uniq = int(col.nunique(dropna=True))
            freq: Any = None
            numeric_stats: Optional[Dict[str, float]] = None

            if np.issubdtype(col.dtype, np.datetime64):
                freq = Counter(col.dt.date.astype('str')).most_common(topn)
            elif np.issubdtype(col.dtype, np.number):
                valid = col.dropna()
                if len(valid):
                    numeric_stats = {
                        'min': float(valid.min()),
                        'max': float(valid.max()),
                        'mean': float(valid.mean()),
                        'sum': float(valid.sum()),
                    }
                freq = Counter(valid.astype('float').round(2).astype('str')).most_common(topn) if len(valid) else []
            else:
                freq = Counter(col.dropna().astype('str')).most_common(topn)

            summary[c] = {
                'rows': n,
                'non_null': non_na,
                'null': na,
                'null_pct': round(na / n * 100, 6) if n else 0.0,
                'unique': uniq,
                'top_values': freq,
                'numeric_stats': numeric_stats,
                'dtype': str(col.dtype),
            }
        return summary

    def derive_group(self, col: str) -> str:
        """æ ¹æ®åˆ—åå…³é”®è¯å½’ç±»"""
        name = str(col)
        if ('æ—¶é—´' in name) or ('æ—¥æœŸ' in name):
            return 'time'
        if 'æœºæ„' in name:
            return 'organization'
        if ('ä¿è´¹' in name) or ('è´¹ç”¨' in name) or ('NCD' in name):
            return 'finance'
        if ('é™©' in name):
            return 'product'
        if ('æ˜¯å¦' in name):
            return 'flag'
        if ('4S' in name) or ('é›†å›¢' in name):
            return 'partner'
        if ('è½¦ç‰Œ' in name):
            return 'vehicle'
        if ('è½¦' in name):
            return 'vehicle'
        return 'general'

    def dtype_to_role(self, dtype_str: str) -> str:
        """dtypeæ˜ å°„ä¸ºrole"""
        if dtype_str.startswith('float') or dtype_str.startswith('int'):
            return 'measure'
        return 'dimension'

    def dtype_to_kind(self, dtype_str: str) -> str:
        """dtypeæ˜ å°„ä¸ºkind"""
        if dtype_str.startswith('float') or dtype_str.startswith('int'):
            return 'number'
        if 'datetime' in dtype_str:
            return 'datetime'
        return 'string'

    def default_aggregation(self, role: str) -> str:
        """é»˜è®¤èšåˆæ–¹å¼"""
        return 'sum' if role == 'measure' else 'none'

    def generate_alias_from_cn(self, col: str) -> tuple[str, bool]:
        """
        æ ¹æ®ä¸­æ–‡åˆ—åç”Ÿæˆè‹±æ–‡åˆ«å
        è¿”å›: (è‹±æ–‡å, æ˜¯å¦åœ¨æ˜ å°„åº“ä¸­æ‰¾åˆ°)
        """
        text = str(col)

        # ä¼˜å…ˆå®Œå…¨åŒ¹é…
        mapping = self.mapping_manager.get_mapping(text)
        if mapping:
            return mapping['en_name'], True

        # çŸ­è¯­åŒ¹é…ï¼ˆè´ªå©ªæœ€é•¿ï¼‰
        phrases = sorted(self.phrase_to_token.items(), key=lambda kv: len(kv[0]), reverse=True)
        tokens: list[str] = []
        for ph, tk in phrases:
            if ph in text and tk not in tokens:
                tokens.append(tk)
                text = text.replace(ph, '')

        if tokens:
            return '_'.join(tokens), True

        return 'unknown_field', False

    def build_field_mapping(self, sheet_summary: Dict[str, Dict[str, Any]]) -> tuple[List[Dict[str, Any]], List[str]]:
        """
        ç”Ÿæˆå­—æ®µæ˜ å°„åˆ—è¡¨
        è¿”å›: (æ˜ å°„åˆ—è¡¨, æœªçŸ¥å­—æ®µåˆ—è¡¨)
        """
        mapping: List[Dict[str, Any]] = []
        used_names: Dict[str, int] = {}
        unknown_fields: List[str] = []

        for idx, (col, s) in enumerate(sheet_summary.items()):
            dtype_str = str(s['dtype'])

            # ä¼˜å…ˆä½¿ç”¨æ˜ å°„åº“ä¸­çš„ä¿¡æ¯ï¼ˆåŒ…æ‹¬è‹±æ–‡åï¼‰
            field_mapping = self.mapping_manager.get_mapping(str(col))
            if field_mapping:
                # âœ… ä½¿ç”¨æ˜ å°„åº“ä¸­çš„è‹±æ–‡åã€groupã€dtypeå’Œdescription
                field_en = field_mapping['en_name']
                group = field_mapping.get('group', self.derive_group(col))
                kind = field_mapping.get('dtype', self.dtype_to_kind(dtype_str))
                desc = field_mapping.get('description', str(col))
                found = True
            else:
                # å¦‚æœæ˜ å°„åº“ä¸­æ²¡æœ‰ï¼Œæ‰ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆ
                field_en, found = self.generate_alias_from_cn(str(col))
                if not found:
                    unknown_fields.append(str(col))

                # ä½¿ç”¨è‡ªåŠ¨æ¨æ–­
                group = self.derive_group(col)
                kind = self.dtype_to_kind(dtype_str)
                # ç”Ÿæˆæè¿°
                if kind == 'number':
                    desc = f"æ•°å€¼å­—æ®µï¼ˆ{col}ï¼‰ï¼Œå¯ç”¨äºæŒ‰æ—¶é—´/æœºæ„ç­‰ç»´åº¦è¿›è¡Œæ±‡æ€»åˆ†æã€‚"
                elif kind == 'datetime':
                    desc = f"æ—¶é—´å­—æ®µï¼ˆ{col}ï¼‰ï¼Œå¯ç”¨äºæ—¶é—´åºåˆ—ç»Ÿè®¡ä¸è¶‹åŠ¿åˆ†æã€‚"
                else:
                    desc = f"åˆ†ç±»/æ–‡æœ¬å­—æ®µï¼ˆ{col}ï¼‰ï¼Œå¯ç”¨äºåˆ†ç»„ä¸ç»´åº¦ç»Ÿè®¡ã€‚"

            role = self.dtype_to_role(dtype_str)

            # ğŸ†• ä¸“ä¸šrole/aggregationè§„åˆ™ï¼ˆè¦†ç›–é»˜è®¤è¡Œä¸ºï¼‰
            col_name = str(col)

            # 1. è¯„åˆ†/ç­‰çº§/åˆ†æ•°/çº§åˆ« å­—æ®µåº”ä¸ºç»´åº¦ï¼ˆdimensionï¼‰ï¼Œè€Œéåº¦é‡
            if any(keyword in col_name for keyword in ['è¯„åˆ†', 'ç­‰çº§', 'åˆ†æ•°', 'çº§åˆ«']):
                role = 'dimension'

            # 2. ç³»æ•°å­—æ®µä¹Ÿåº”ä¸ºç»´åº¦æˆ–ä½¿ç”¨å¹³å‡å€¼èšåˆ
            if 'ç³»æ•°' in col_name:
                role = 'dimension'

            # 3. æ¯”ä¾‹/æŠ˜æ‰£/ç³»æ•°å­—æ®µå¦‚æœæ˜¯åº¦é‡ï¼Œåº”ä½¿ç”¨å¹³å‡å€¼èšåˆï¼ˆä¸åº”æ±‚å’Œï¼‰
            aggregation = self.default_aggregation(role)
            if role == 'measure':
                if any(keyword in col_name for keyword in ['æ¯”ä¾‹', 'æŠ˜æ‰£', 'ç³»æ•°', 'NCD', 'ä¼˜å¾…']):
                    aggregation = 'avg'
                # ä¿è´¹/é‡‘é¢/è´¹ç”¨ç­‰ç¡®ä¿ä½¿ç”¨sumï¼ˆè¿™æ˜¯é»˜è®¤å€¼ï¼Œä½†æ˜¾å¼ç¡®è®¤ï¼‰
                elif any(keyword in col_name for keyword in ['ä¿è´¹', 'é‡‘é¢', 'è´¹ç”¨', 'ä»·æ ¼', 'èµ”æ¬¾', 'æ‰‹ç»­è´¹', 'ç¨']):
                    aggregation = 'sum'

            # ç¡®ä¿è‹±æ–‡åå”¯ä¸€
            if field_en in used_names:
                used_names[field_en] += 1
                field_en = f"{field_en}_{used_names[field_en]}"
            else:
                used_names[field_en] = 0

            notes = []
            null_pct = s.get('null_pct', 0.0)
            if null_pct > 0:
                notes.append(f"ç©ºå€¼ç‡çº¦ {null_pct}%")
            if kind == 'number' and s.get('numeric_stats'):
                ns = s['numeric_stats']
                if ns['min'] < 0:
                    notes.append("å­˜åœ¨è´Ÿæ•°ï¼Œå¯èƒ½ä¸ºå†²é”€/é€€æ¬¾/æ‰¹æ”¹")

            mapping.append({
                'field_name': field_en,
                'cn_name': str(col),
                'source_column': str(col),
                'group': group,
                'dtype': kind,
                'role': role,
                'aggregation': aggregation,
                'description': desc,
                'notes': 'ï¼›'.join(notes) if notes else '',
                'is_mapped': found
            })

        return mapping, unknown_fields

    def html_escape(self, text: str) -> str:
        """HTML è½¬ä¹‰"""
        return (
            str(text)
            .replace('&', '&amp;')
            .replace('<', '&lt;')
            .replace('>', '&gt;')
        )

    def build_html_report(self, xlsx_path: Path, sheets: list, summaries: Dict[str, Dict[str, Dict[str, Any]]], topn: int) -> str:
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        file_type = "CSV" if self.is_csv_file(xlsx_path) else "Excel"
        title = f"{file_type} å­—æ®µåˆ†ææŠ¥å‘Š"
        generated_at = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        head = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>{self.html_escape(title)}</title>
  <style>
    body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, 'Noto Sans', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif; margin: 24px; color: #1e293b; }}
    h1 {{ font-size: 22px; margin-bottom: 8px; }}
    h2 {{ font-size: 18px; margin-top: 24px; margin-bottom: 8px; }}
    .meta {{ color: #475569; font-size: 14px; margin-bottom: 16px; }}
    table {{ border-collapse: collapse; width: 100%; margin: 8px 0 24px; font-size: 14px; }}
    th, td {{ border: 1px solid #cbd5e1; padding: 8px; text-align: left; vertical-align: top; }}
    th {{ background: #f1f5f9; font-weight: 600; }}
    .code {{ font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', monospace; background: #f8fafc; border: 1px solid #e2e8f0; padding: 4px 6px; border-radius: 4px; }}
  </style>
</head>
<body>
  <h1>{self.html_escape(title)}</h1>
  <div class="meta">æ•°æ®æ–‡ä»¶ï¼š<span class="code">{self.html_escape(str(xlsx_path))}</span> | ç”Ÿæˆæ—¶é—´ï¼š{self.html_escape(generated_at)} | å·¥ä½œè¡¨æ•°é‡ï¼š{len(sheets)}ï¼ˆ{self.html_escape(', '.join(sheets))}ï¼‰ | Top å€¼å±•ç¤ºï¼šå‰ {topn} é¡¹</div>
"""
        parts = [head]

        for sheet in sheets:
            parts.append(f"<h2>å·¥ä½œè¡¨ï¼š{self.html_escape(sheet)}</h2>")
            parts.append("<table>")
            parts.append("<thead><tr>\n"
                         "<th>åˆ—å</th>\n"
                         "<th>è¡Œæ•°</th>\n"
                         "<th>éç©º</th>\n"
                         "<th>ç©ºå€¼</th>\n"
                         "<th>ç©ºå€¼ç‡</th>\n"
                         "<th>å”¯ä¸€å€¼</th>\n"
                         "<th>ç±»å‹</th>\n"
                         "<th>æ•°å€¼ç»Ÿè®¡</th>\n"
                         "<th>Top å€¼</th>\n"
                         "</tr></thead>")
            parts.append("<tbody>")

            summary = summaries[sheet]
            for col_name, s in summary.items():
                stats = s.get('numeric_stats')
                stats_str = ''
                if stats:
                    stats_str = f"min={stats['min']:.4f}; max={stats['max']:.4f}; mean={stats['mean']:.4f}; sum={stats['sum']:.4f}"
                tv = s.get('top_values') or []
                tv_str = ', '.join([f"{self.html_escape(v)}({cnt})" for v, cnt in tv])
                parts.append(
                    "<tr>" +
                    f"<td>{self.html_escape(col_name)}</td>" +
                    f"<td>{s['rows']}</td>" +
                    f"<td>{s['non_null']}</td>" +
                    f"<td>{s['null']}</td>" +
                    f"<td>{s['null_pct']}%</td>" +
                    f"<td>{s['unique']}</td>" +
                    f"<td>{self.html_escape(s['dtype'])}</td>" +
                    f"<td>{self.html_escape(stats_str)}</td>" +
                    f"<td>{tv_str}</td>" +
                    "</tr>"
                )
            parts.append("</tbody>")
            parts.append("</table>")

        parts.append("</body></html>")
        return '\n'.join(parts)

    def analyze_excel(self, xlsx_path: str, output_dir: str, topn: int = 10) -> Dict[str, Any]:
        """åˆ†æExcelæˆ–CSVæ–‡ä»¶"""
        try:
            xlsx_path = Path(xlsx_path)
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)

            print(f"ğŸ“Š å¼€å§‹åˆ†æ: {xlsx_path.name}")

            # æ£€æµ‹æ–‡ä»¶ç±»å‹
            if self.is_csv_file(xlsx_path):
                # CSVæ–‡ä»¶ï¼Œè§†ä¸ºå•ä¸ªå·¥ä½œè¡¨
                sheets = [xlsx_path.stem]  # ä½¿ç”¨æ–‡ä»¶åä½œä¸ºå·¥ä½œè¡¨å
                print(f"ğŸ“„ æ–‡ä»¶ç±»å‹: CSV")
            else:
                # Excelæ–‡ä»¶ï¼ŒåŠ è½½å·¥ä½œç°¿
                xls = pd.ExcelFile(xlsx_path)
                sheets = xls.sheet_names
                print(f"ğŸ“„ å·¥ä½œè¡¨: {', '.join(sheets)}")

            summaries: Dict[str, Dict[str, Dict[str, Any]]] = {}
            total_rows = 0
            total_cols = 0

            for sheet in sheets:
                if self.is_csv_file(xlsx_path):
                    # CSVæ–‡ä»¶ä¸éœ€è¦sheet_nameå‚æ•°
                    df = self.load_sheet(xlsx_path)
                else:
                    # Excelæ–‡ä»¶éœ€è¦sheet_nameå‚æ•°
                    df = self.load_sheet(xlsx_path, sheet)

                summary = self.summarize_columns(df, topn=topn)
                summaries[sheet] = summary
                total_rows += len(df)
                total_cols += len(df.columns)

            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            base = xlsx_path.stem
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

            html_filename = f"{base}_{timestamp}_åˆ†ææŠ¥å‘Š.html"
            json_filename = f"{base}_{timestamp}_å­—æ®µæ˜ å°„.json"

            html_path = output_dir / html_filename
            json_path = output_dir / json_filename

            # ç”ŸæˆHTMLæŠ¥å‘Š
            html = self.build_html_report(xlsx_path, sheets, summaries, topn)
            html_path.write_text(html, encoding='utf-8')
            print(f"âœ… HTMLæŠ¥å‘Š: {html_path}")

            # ç”Ÿæˆå­—æ®µæ˜ å°„
            first_sheet = sheets[0]
            field_map, unknown_fields = self.build_field_mapping(summaries[first_sheet])

            # ğŸ¤– AIæ‰¹é‡å­¦ä¹ æœªçŸ¥å­—æ®µ
            if unknown_fields and AI_MAPPER_AVAILABLE:
                print(f"\nğŸ” å‘ç° {len(unknown_fields)} ä¸ªæœªçŸ¥å­—æ®µ")
                print("ğŸ’¡ ä½¿ç”¨AIè‡ªåŠ¨ç”Ÿæˆå­—æ®µæ˜ å°„...")

                # è·å–DataFrameç”¨äºæ ·æœ¬åˆ†æ
                if self.is_csv_file(xlsx_path):
                    df_for_learning = self.load_sheet(xlsx_path)
                else:
                    df_for_learning = self.load_sheet(xlsx_path, first_sheet)

                # AIæ‰¹é‡å­¦ä¹ 
                learned_mappings = self.mapping_manager.batch_learn_fields(unknown_fields, df_for_learning)

                if learned_mappings:
                    print(f"âœ… å·²ç”Ÿæˆ {len(learned_mappings)} ä¸ªå­—æ®µæ˜ å°„å¹¶ä¿å­˜åˆ° custom.json")

                    # é‡æ–°ç”Ÿæˆå­—æ®µæ˜ å°„ï¼ˆåŒ…å«æ–°å­¦ä¹ çš„å­—æ®µï¼‰
                    field_map, unknown_fields = self.build_field_mapping(summaries[first_sheet])

            json_path.write_text(json.dumps(field_map, ensure_ascii=False, indent=2), encoding='utf-8')
            print(f"âœ… å­—æ®µæ˜ å°„: {json_path}")

            # ğŸ” æ˜ å°„è´¨é‡æ£€æŸ¥
            quality_report_path = None
            if VALIDATOR_AVAILABLE and field_map:
                print(f"\nğŸ” è¿›è¡Œæ˜ å°„è´¨é‡æ£€æŸ¥...")
                validator = MappingValidator()
                validation_result = validator.batch_validate(field_map)

                # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
                quality_report_filename = f"{base}_{timestamp}_è´¨é‡æ£€æŸ¥æŠ¥å‘Š.md"
                quality_report_path = output_dir / quality_report_filename
                validator.generate_report(validation_result, quality_report_path)

                stats = validation_result['stats']
                print(f"âœ… è´¨é‡æ£€æŸ¥å®Œæˆ:")
                print(f"   å¹³å‡åˆ†: {stats['avg_score']}/100")
                print(f"   ä¼˜ç§€: {stats['excellent']}  è‰¯å¥½: {stats['good']}  ä¸€èˆ¬: {stats['fair']}  è¾ƒå·®: {stats['poor']}")
                if stats['needs_review']:
                    print(f"   âš ï¸  éœ€è¦å®¡æ ¸: {len(stats['needs_review'])} ä¸ªå­—æ®µ")
                print(f"   æŠ¥å‘Š: {quality_report_path}")

            # ç»Ÿè®¡ä¿¡æ¯
            field_stats = {
                'total_fields': len(field_map),
                'by_dtype': {},
                'by_group': {},
                'unknown_count': len(unknown_fields),
                'mapped_count': len(field_map) - len(unknown_fields)
            }

            for field in field_map:
                dtype = field['dtype']
                group = field['group']
                field_stats['by_dtype'][dtype] = field_stats['by_dtype'].get(dtype, 0) + 1
                field_stats['by_group'][group] = field_stats['by_group'].get(group, 0) + 1

            result = {
                'success': True,
                'message': f'æˆåŠŸåˆ†æ {len(sheets)} ä¸ªå·¥ä½œè¡¨',
                'sheets': sheets,
                'total_rows': total_rows,
                'total_cols': total_cols,
                'html_path': str(html_path),
                'json_path': str(json_path),
                'field_stats': field_stats,
                'unknown_fields': unknown_fields,
                'topn': topn
            }

            # æ·»åŠ è´¨é‡æŠ¥å‘Šè·¯å¾„
            if quality_report_path:
                result['quality_report_path'] = str(quality_report_path)

            return result

        except Exception as e:
            return {
                'success': False,
                'message': f'åˆ†æå¤±è´¥: {str(e)}'
            }


if __name__ == '__main__':
    import sys

    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python analyzer.py <æ–‡ä»¶è·¯å¾„(Excel/CSV)> [è¾“å‡ºç›®å½•] [topn]")
        print("æ”¯æŒæ ¼å¼: .xlsx, .xls, .csv, .txt")
        sys.exit(1)

    file_path = sys.argv[1]
    output_dir = sys.argv[2] if len(sys.argv) > 2 else './output'
    topn = int(sys.argv[3]) if len(sys.argv) > 3 else 10

    analyzer = ExcelAnalyzer()
    result = analyzer.analyze_excel(file_path, output_dir, topn)

    if result['success']:
        print(f"\nâœ… åˆ†æå®Œæˆï¼")
        print(f"æœªçŸ¥å­—æ®µæ•°: {len(result['unknown_fields'])}")
        if result['unknown_fields']:
            print(f"æœªçŸ¥å­—æ®µ: {', '.join(result['unknown_fields'])}")
    else:
        print(f"\nâŒ {result['message']}")
